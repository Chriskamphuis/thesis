\chapter{Related Work}
\label{related-work}

\epigraph{Machines take me by surprise with great frequency.}{Alan Turing - 1950}

\section{Introduction}
This chapter aims to provide the context in which this work is written. By introducing the related scientific fields, this chapter sketches a broader context wherein this research exists. 

First, the field of information retrieval is introduced, the central area of the research described in this dissertation. Within the field of information retrieval, different approaches to \emph{retrieving information} have been used throughout the decades. Different approaches will be introduced such that the reader understands what works in the field.

Secondly, techniques from different research areas are also used for this research. Specifically, the research described in this paper extensively uses techniques typically investigated by the \emph{data management} community. This field will also be introduced, especially the techniques used for the research described in this work. 

Then, the concept of \emph{graphs} will be explained. Graphs themselves are just mathematical structures that model the relations between objects. The objects are modeled as \emph{nodes} and relations between them as \emph{edges}. Many kinds of graphs exist, and some are very useful for modeling problems that can be expressed formally through this framework. Different kinds of graphs will be explained, and examples of how to use them will be provided.

Finally, we will describe the idea of \emph{reproducible science}. In general, scientific work must be reproducible. In our research, we spend additional effort on reproducible science. What reproducibility exactly entails will be introduced.  

\section{Information Retrieval}
The research described in this thesis is subject to the \emph{information retrieval} field. Colloquially, I would refer to this field as the science of everything related to search engines. This field encompasses all aspects: from user experiences with search engines to storage algorithms and fast retrieval of the information items users search. \Citet{modern-information-retrieval} introduces information retrieval as the following:

\medskip
\textbf{Information retrieval (IR) deals with the representation, storage, organization of, and access to information items.}

\medskip
Following this description, an information retrieval system, i.e., a search engine, is a system that allows users to access information items that they are looking for. How this works internally is typically not interesting for the user, they want to find the information they seek. Typically, and also in this thesis, the term \emph{document} refers to information items, even though the item the user seeks would not be a document in the ``literal sense''. 

Consider the following situation. If one wants to know whether it will rain in the coming half an hour, they might query a web search engine with the text \emph{weather}. In this case, the user does not care how the search engine works but only about the result. In order to satisfy the user's information need, the search engine needs to (1) present the correct weather forecast and (2) do this quickly. If the search engine presents the incorrect weather forecast or cannot do this in seconds, the user will be dissatisfied.

In order to answer this inquiry for information correctly, the information retrieval system needs more context. It can only correctly know the weather if it is known where the user resides. Typically, when accessing search engines through a telephone or computer, this information is sent along with the query as metadata. This way, the search engine can correctly answer even though the user did not explicitly give this information. 

Then when the user's location is known, the search engine needs to find a webpage that correctly provides the weather for that particular location. As there are billions of web pages in the search engine's index, correctly identifying which one contains information about the weather at that particular location and time and then retrieving it in seconds is the next challenge.

Search engines must smartly organize the data because it is infeasible to iterate over all web pages to check if the correct information is available. Typically, the organization of documents within a search engine is done through a data structure called an inverted index.

\subsection{Inverted Indexes}
Inverted indexes have been used for decades in the field of information retrieval. They are designed to access documents quickly. Otherwise, the user would use a different system. Documents are typically structured through words that form sentences, which form paragraphs, and, eventually, stories. When considering a document in its standard form, it is not trivial to quickly know whether it contains a specific word. When searching for something, it is paramount that the search engines know what documents contain the keywords in the query. To store this information, the documents are \emph{inverted}. Instead of storing documents as is; a mapping from documents to words, the search engines store the inverted information; a mapping from words to documents. To provide an example, let us says we have three short documents:

\begin{itemize}
	\item[\textbf{1.}] Cats and dogs are animals.
	\item[\textbf{2.}] Cats are smart animals.
	\item[\textbf{3.}] Dogs are great at tricks. 
\end{itemize}
Then the inverted index looks something like this:

\medskip
\textbf{animal:} [1 2],
\textbf{cat:} [1 2], 
\textbf{dog:} [1 3],
\textbf{great:} 3,
\textbf{smart:} 2,
\textbf{trick:} 3
\medskip

\noindent There are a couple of interesting observations that can be made here. Not all words appear in the inverted index. Typically words without ``meaning'', or so-called stop words, are dropped. These words tend to contain no information and can therefore be dropped. This process is called stopword removal or \emph{stopping}. Then, words are sometimes shortened to their \emph{stem}. Let us say we have a query only mentioning the word ``dog'', so not plural, then we still want to be able to find the documents that mention the plural form. This shortening to the word's stem is called \emph{stemming}. In this case, the words are ordered alphabetically. When the number of documents in a collection increases, the number of unique words also increases. By storing the words alphabetically, a computer can find the entries for that particular word easier. 

When someone queries the search engine with following query: ``dog tricks'', then can directly find which documents contain these words, automatically discarding all documents that do not contains these words. This is done by retrieving the lists associated with these words. These lists are called \emph{posting lists} in the field of information retrieval. Entries in such lists are referred to as \emph{postings}. In this case the postings only contain the document identifiers as data. Generally, however, information like how often a word appears in a document, or its location in the document is also stored in the posting. In practice, posting lists are often compressed and instead of storing the direct identifiers, the gaps between them are stored such that the index becomes smaller. In literature these gaps are referred to as delta-gaps. 

In this case the posting lists of the words ``dog'' and ``trick'' are retrieved, which are [1 3] and [3]. Then some scoring method can process these lists and assign a score to the documents present in these lists. As document 3 is the only document that contains both words, it makes sense to consider this document as the most relevant. However, when multiple documents contain both words with different frequencies, it is not as trivial to assess a relevance ordering. 
To create such an ordering, different models for scoring documents have been proposed in the past. In the following sections these models are presented.

\subsection{Ranking Models}

\subsubsection{Boolean Retrieval}
In the early days of information retrieval, the boolean retrieval model was used. Boolean retrieval can be formulated as the following; let, 

\begin{equation}
	T = \begin{Bmatrix}
		t_1, & t_2, & \ldots, & t_n
	\end{Bmatrix}
\end{equation}
be the set of all index terms that appear in the collection. Let,
\begin{equation}
	D = \begin{Bmatrix}
		d_1, & d_2, & \ldots, & d_m
	\end{Bmatrix}
\end{equation}

be the set of all documents, where every document is a subset of $T$. Then a query can be any boolean expression over $T$. All documents that adhere to the boolean expression formed by the query are considered relevant. To give an example, consider the same three documents as before:

\begin{itemize}
	\item[\textbf{1.}] Cats and dogs are animals.
	\item[\textbf{2.}] Cats are smart animals.
	\item[\textbf{3.}] Dogs are great at tricks. 
\end{itemize}
Then, $D$ is formulated as $\left\{
\begin{smallmatrix}
		d_1, & d_2, & d_3 
\end{smallmatrix}
\right\}$ with,
\begin{align}
d_1&: \begin{Bmatrix}
	\mathit{cat}, & \textit{dog}, & \textit{animal}
\end{Bmatrix}\\
d_2&: \begin{Bmatrix}
	\mathit{cat}, & \textit{smart}, & \textit{animal}
\end{Bmatrix}\\
d_3&: \begin{Bmatrix}
\mathit{dog}, & \textit{great}, & \textit{trick}
\end{Bmatrix}
\end{align}

If would be interested in finding all documents that mention dogs, but not cats. Then we can formulate the following boolean query:

\begin{equation}
	Q = \mathit{dog} \land \neg \mathit{cat}
\end{equation}

Then if we find all subsets that adhere to the individual terms in this conjunction we get the following set expression:
\begin{equation}
Q = \{d_1, d_3\}\ \cap \{d_3\} = \{d_3\}
\end{equation}
which shows that document $d_3$ is the only document that is about dogs, but not about cats. Although it is possible to express quite complicated expressions with boolean logic, a document either satisfies the expression or it does not. All documents that satisfy the expression are considered relevant; creating a ranking between them cannot be done with this approach alone. 

With this approach there is no clear difference, yet, between data retrieval and information retrieval; there is only one correct solution, namely all documents that satisfy the restrictions imposed by the query. 

\subsubsection{Vector Space Models}
In 1975, the vector space model was introduced by~\citet{VectorSpaceModel}. The idea behind the vector space model is to represent documents and queries as vectors. Consider a collection that has $t$ unique terms, then we can represent a documents in the collection as a $t$ dimensional vector:
\begin{equation}
	D_j = \begin{pmatrix}
		w_0, \cdots, w_t
	\end{pmatrix}
\end{equation}
where $D_j$ is the $j$-th document in the collection, and $w_i$ represents the weight associated with the $i$-th term. These weights can be binary, if one only wants to consider the presence of the term in the document, they can be the term frequency, or any weight that takes into account the ``general'' term importance. 

Then if we have a query $q$, we can represent it in the same way:
\begin{equation}
	q = \begin{pmatrix}
		q_0, \cdots, q_t
	\end{pmatrix}
\end{equation}
where $q_i$ represents the weight of the $i$-th term in the collection. Most values in these vectors will be zero.

Now it is possible to calculate a similarity score between the query and every document. This similarity is typically calculated by calculating the cosine similarity between the two vectors, which between a document $d$ and a query $q$ is defined as:

\begin{equation}
	\cos\left(d, q\right) = \frac{\textbf{d} \cdot \textbf{q}}{||\textbf{d}||\ ||\textbf{q}||}
\end{equation}

Then, calculating this for the documents in the collection gives a relevance score value for all of them. Ordering them based on relevance, then produces a ranked list where the highest is presumed to be the most interesting for the user. As mentioned before, the weights represented in the document vector do not necessarily have to be binary or the term frequency of the terms in the document. \Citeauthor{VectorSpaceModel} showed that the product of the term frequency with a general term importance worked best. The measure they used for term importance was the inverse document frequency as proposed by \citet{idf} three years earlier, which worked well. 

\subsubsection{Probabilistic Relevance Models}
In 1976,~\citeauthor{RSJ} developed the probabilistic ranking framework. Within this framework, we assume that a document has a certain probability of being relevant given a query. Then, a search systems that implements models within this framework, rank documents where documents with the highest probability of being relevant are ranked the highest.

Within this framework, with the assumption of binary independence, which is nicely explained by ~\citet{bm25-beyond}, the Robertson-Sp{\"a}rck Jones weight can be derived. Which, when assuming no relevance information is available, leads to an approximation of the classical inverse document frequency.

Then by extending this function to include within-document term frequency information and document length normalization, the well known BM25 function can be derived.
This model is the focus of \Cref{ir-using-relational-databases}, and additional information explaining this model will be presented there.

\subsubsection{Language Models}
Later, language model were proposed~\citep{croft_lm, hiemstra_lm, zhai_lm}. The idea behind language models that documents are considered language samples. These samples can be used for a generative process. This process generates terms that exists in the sample by randomly selecting one of the words from that sample. Let the probability of a document $d$ generating a term $t_i$ be:

\begin{equation}
	P(t_i|d) = \frac{\mathit{tf}_{t_i,d}}{\sum_t tf(t, d)}
\end{equation}
with $\mathit{tf}_{t_j, d}$ being the term frequency of term $t_j$ in document $d$. 

Then, if we have a query $Q$. It is possible to calculate the likelihood that this query was generated by the document samples, as we know for every term in the query what the probability is that one of the document samples generates that term: 

\begin{equation}
	P(d|Q) = \prod_{q \in Q} P(q | d)
\end{equation}

When we would just take the document with the maximum likelihood, there are some issues we run into. First of all, if a query contains multiple terms, a document can only get a non-zero likelihood for generating that query if all query terms exist in that document. If this is not the case the probability of generating that term would be 0, and as we are calculating the maximum likelihood we would take the product of the individual probabilities of the query terms. Then the second issue is that there is no term independent specificity weighting, terms that have a higher degree of specificity should increase the resulting probabilities more than ``filler'' words. The previously described inverse document frequency took care of this in the vector space model.

In order to overcome these issues, a smoothing technique tends to be used. Instead of only considering the samples formed by the documents, a collection wide sample is also being used. This collection wide sample generates terms according to the frequency in which the terms appear in the collection:
\begin{equation}
	P(t_i|C) = \frac{\mathit{df}(t_i)}{\sum_t \mathit{df}(t)}
\end{equation}
With $C$ being the collection, and $df(t_i)$ the document frequency of term $t_i$; i.e.\ the number of documents in the collection in which term $t_i$ appears. Then, the probability generated by the document sample is interpolated by the probability generated by the collection sample:
\begin{equation}
	P(d|Q) = \prod_{q \in Q} \omega \cdot P(q | d) + (1 - \omega) \cdot P(q | C) 
\end{equation}
with $\omega$ being the smoothing factor; i.e.\ how much such the collection sample be weighted against the document sample. 

This smoothing solves both problems. As the collection sample contains all terms in the collection, it is not possible anymore for probabilities generated by a certain term to be zero, so the resulting product will not be zero either. Then, words with a higher specificity also get a boost; words with a high specificity have a low probability of being generated by the collection sample. This means that it is more important for a document to contain that word in order to achieve a high probability of relevance. Words that have a low specificity have less effect; the probability of them being generated by the collection sample is already relatively high. 

\subsubsection{Learning to Rank}
With the increase of computing power, learning to rank became a popular approach to ranking. The methods described in the previous sections, can all be considered unsupervised learning methods if we look at from a machine learning perspective. Learning to rank models would be considered supervised or reinforcement learning methods. 

The goal is to learn a model, that given a query ranks documents that are relevant to that query higher than documents that are not relevant to that query. In general, there are three ways to achieve this, for all these methods it's assumed that relevance labels are available from which the function can be learned. Learning-to-rank methods are usually categorized in the following three categories: 

\paragraph{Pointwise.} Pointwise methods are most comparable to standard regression methods. The idea here is to learn a function that tries to predict the relevance label of a document directly. Consider that we have $n$ query-document pairs for which relevance labels are available, with $m$ independent variables per pair. Then the relevance labels for these pairs can be expressed as a vector $\mathbf{y}$, and the independent variables as a matrix $\mathbf{X}$;
\begin{multicols}{2}
	\begin{equation}
		\mathbf{y} = 
		\begin{bmatrix}
			y_1 \\
			\vdots \\
			y_n
		\end{bmatrix}
	\end{equation}\break
	\begin{equation}
		\mathbf{X} = 
		\begin{bmatrix}
			x_{1,1} & \cdots & x_{1, m} \\
			\vdots &  \ddots & \vdots \\
			x_{n,1} & \cdots & x_{n, m} 
		\end{bmatrix}
	\end{equation}	
\end{multicols}
Then, the goal is to learn a function $\mathbf{\hat y} = f(\mathbf{X})$ such that some distance measure between $\mathcal{L}\left(\mathbf{\hat y}, \mathbf{y} \right)$ is minimized. Commonly used measures for this are mean square error or mean absolute error. The pointwise learning approach to ranking has some unwanted properties however. Consider a situation where we have three documents; 
$\left[
\begin{smallmatrix}
	d_1, & d_2, & d_3
\end{smallmatrix}
\right]$, with relevance labels
$\left[
\begin{smallmatrix}
	1, & 0, & 0
\end{smallmatrix}
\right]$, i.e.\ document $d_1$ is relevant and the other documents are not. Then, two different ranking functions, $a$ and $b$, might produce the following predicted relevance labels: 
$a = \left[
\begin{smallmatrix}
	1, & 0.9, & 0.8
\end{smallmatrix}
\right]$ 
and
$b = \left[
\begin{smallmatrix}
	0, & 0.1, & 0.2
\end{smallmatrix}
\right]$, which results in the following respective rankings:
$\left[
\begin{smallmatrix}
	d_1, & d_2, & d_3
\end{smallmatrix}
\right]$ 
and 
$\left[
\begin{smallmatrix}
	d_3, & d_2, & d_1
\end{smallmatrix}
\right]$. 
The ranking produced by system $a$ is objectively better, it ranks the relevant document as the most relevant. System $b$ however ranks the relevant document as the least relevant. If we however would evaluate the two different system with a loss functions, say mean absolute error, system $b$ would seem better as the means absolute error is smaller. 

\paragraph{Pairwise.} Pairwise ranking tries to deal with this by not directly learning from the relevance label, but by looking at pairs of documents. The idea behind this is that it does not matter what the actual ranking score value is determined to be, as long as this score is higher for relevant documents compared to the score of non relevant documents.  

Pairwise functions consider, given a query, two documents at a time. Then the function tries which one of the two documents is more relevant than the other. If we consider the same set of relevance labels $\mathbf{y}$ and independent variables $\mathbf{X}$, then if we would take a pair of documents $x_k$ and $x_l$, with their corresponding relevance labels $y_k$ and $y_l$, assuming $y_k \neq y_l$, then we are trying to learn the following function:
\begin{equation}
	f(x_k, x_l) = \begin{cases}
		0 & \text{if } y_k < y_l \\
		1 & \text{if } y_k > y_l
	\end{cases}
\end{equation}
This function can be learned by any binary classification method. After the method is learned, it can be used to determine an ordering of documents. 
Pairwise methods tend to be more expensive to train than pointwise methods, as the number of samples to train on for pointwise methods are $n$, with $n$ being the number of documents for which relevance assessments are available. For pairwise methods, every document with an label can be compared with every other document, creating $n^2$ samples. 

A disadvantage of pairwise methods is that every pair is treated equally when training a model. There is no difference when comparing the two highest ranked documents compared to comparing the two lowest ranked documents, even though users typically care about the highest ranked documents. 

\paragraph{Listwise.} Instead of only looking at pairs of documents, the listwise approach tries to take the whole ranking as a whole into account. This way, the top of the ranking can effect the learning of the function more than the bottom of the ranking.
Generally, when learning a listwise learning-to-rank method, the goal is to optimize some quality of ranking metric directly. Examples of such quality of ranking methods will be introduced in~\cref{sec:evaluation}.

\paragraph{Multistage Ranking}
In many cases it is still not possible to apply inference to all documents in the collection. In practice, often a non learning-to-rank approach tends to be used to create an initial ranking. Then it is assumed that the top-$k$ documents probably contain all relevant documents. Then the learned method only has to reorder the top-$k$ documents to present to the user. 

In these cases it can make sense rank the top-$k$ documents using an inverted index, while the reranking is done with another computer program.
 
\subsubsection{Vector Space Models revisited}
In the last couple of years the vector space model has become more popular again. With the rise of large language models, the vector space model became more popular again due what is called \emph{dense} retrieval.  

The basic idea is to predict the relevance of a document $d$ for a query $q$:
\begin{equation}
	\mathit{rel}(q, d) = \omega\left(\phi\left(q\right),
	\psi\left(d\right)\right)
\end{equation}
Here, $\psi$ and $\phi$ are learned function through the use of large language models that map the query and the document to vectors with a low dimensionality. These functions map the query and the document to the same space. When documents are relevant to the query, the resulting vectors should be similar, while if the document is not relevant they should be dissimilar. This similarity is measured through the function $\omega$, which in many cases is either the dot-product or cosine similarity.

The process of finding the vectors that are the most close to a query vectors is the same as nearest neighbor search. In the case of dense retrieval dedicated dense retrieval systems tend to be used, instead of an inverted index. 

\subsection{Evaluation}
\label{sec:evaluation}
In the previous section all kinds of ranking approaches have been introduced. When comparing different ranking methods to each other we need to be able to measure some kind of ranking quality. In order to measure the quality of rankings produced by systems many different metrics have been introduced. In this section the most common evaluation metrics are being introduced, among them all metrics used in this thesis. 

All metrics introduced are $@k$ metrics. We only evaluate the documents ranked up to rank $k$, i.e.\ if $k=30$, we only consider the first $30$ documents produced by the system.

\subsubsection{Precision}
A straightforward approach of evaluating a ranking produced by a system is through precision. The precision metric counts the number of relevant documents found by the system, which is then divided by the total number of documents found by the system:
\begin{equation}
	\textit{P}@k = \frac{\sum_{i=1}^k\text{rel}\left(d_i\right)}{k}
\end{equation}
In this case $\text{rel}(.)$ is a function that returns 1 if the document is relevant, otherwise 0.
If $k=30$, then only the first $30$ documents are considered. If ten of these documents are relevant, then $P@30 = \frac{10}{30} = \frac{1}{3}$. Precision can however be somewhat limited in its use. In this example, the quality of ranking might not seem good. Only a third of the produced documents are relevant. However, when there are only 10 relevant documents present in the collection, the $P@30$ could not have been higher. 

\subsubsection{Recall}
To prevent the issue that the precision is low when there are not many relevant documents in the collection. It is also possible to measure how many relevant documents were retrieved, compared to the total number of documents. This measure is called recall: 
\begin{equation}
	\textit{R}@k = \frac{\sum_{i=1}^k\text{rel}\left(d_i\right)}{\sum_{i=1}^N\text{rel}\left(d_i\right)}
\end{equation}
Here $N$ is the number of documents in the collection. If, again, $k=30$ and we found $10$ relevant documents, while there are $15$ relevant documents in the collection. Then $R@30 = \frac{10}{15} = \frac{2}{3}$.

\subsubsection{$F_1$-measure}
Where precision measures the share of documents found that are relevant, recall measures the number of relevant documents found compared to the total number of relevant documents. For precision it makes sense to retrieve as few as possible documents; the probability of finding a relevant documents decreases the more documents are returned. For recall it makes sense to retrieve more documents, as when more documents are retrieved it can only increase.  

To find the correct balance between precision and recall, it is possible to use the $F_1$-measure. It is defined as the harmonic mean between precision and recall:
\begin{equation}
	\textit{F}_1@\textit{k}  = 2\cdot\frac{P@k \cdot R@k}{P@k + R@k} 
\end{equation}
if either recall or precision is low, the $F_1$-measure suffers as well. To achieve a high $F_1$-measure, both recall and precision need to be high.

\subsubsection{Average Precision}
Consider the situation where three documents are retrieved, two systems might produce rankings with following relevance labels: $\left[
\begin{smallmatrix}
	1, & 0, & 0
\end{smallmatrix}
\right]$ and $\left[
\begin{smallmatrix}
	0, & 0, & 1
\end{smallmatrix}
\right]$.
Both rankings achieve the same $P@3$ and $R@3$ scores, and subsequently also the same $F_1@3$. In this situation the previously introduced metrics can not distinguish between ranking quality. In this situation it is however clear that the first ranking is better than the second ranking. 
Instead of measuring the precision only at rank $k$, the average precision measure calculates it every time a relevant document is encountered. The sum of these values is divided by the number of relevant documents found up to $k$:
\begin{equation}
	\textit{AP}@k = \frac{1}{\sum_{i=1}^N\text{rel}\left(d_i\right)}\sum^k_{i=1} P\left(i\right) \cdot \text{rel}\left(d_i\right)
\end{equation}
Looking back at the two rankings, 
$\left[
\begin{smallmatrix}
	1, & 0, & 0
\end{smallmatrix}
\right]$ and $\left[
\begin{smallmatrix}
	0, & 0, & 1
\end{smallmatrix}
\right]$, the rankings produce an $\textit{AP}@3$ of $1$ and $\frac{1}{3}$ respectively.

\subsubsection{Discounted Cumulative Gain}
Discounted cumulative gain is an evaluation metric developed after the observation that users often only look at the highest ranked documents, and that documents might not have binary relevance labels.
If a document is ranked at, say rank 5, it might already be possible that it is not considered by the user anymore and that they already reformulated the query. Plus, the metrics described before would rank the following two rankings to be of equal quality: 
$\left[
\begin{smallmatrix}
	2, & 1, & 0
\end{smallmatrix}
\right]$ and $\left[
\begin{smallmatrix}
	1, & 2, & 0
\end{smallmatrix}
\right]$
So discounted cumulative gain was developed with the intention that there should be a discounted gain every time a new relevant document is found, plus non binary relevance labels can be taken into account. This discount is applied by dividing by the logarithm of the rank plus one: 
\begin{equation}
	\textit{DCG}@k = \sum^k_{i=1} \frac{\text{rel}(d_i)}{\log_2(i+1)}
\end{equation}
In this case $\text{rel}(.)$ does not return a binary value, but the actual relevance assessment.

\subsubsection{Normalized Discounted Cumulative Gain}
A disadvantage of $\textit{DCG}$ is that the scores produced by this metric can vary a lot between rankings. If there are many relevant documents present for a query the values $\textit{DCG}$ can take are much higher than if there is only one relevant label available. Normalized discounted cumulative gain takes care of this by dividing by the highest possible $\textit{DCG}$ that can be achieved. So normalized discounted cumulative gain is calculated by dividing the $\textit{DCG}$ of the ranking, by the $\textit{DCG}$ of the ideal ranking:  
\begin{equation}
	\textit{NDCG}@k = \frac{\textit{DCG}@k}{\textit{IDCG}@k} 
\end{equation}
with $\textit{IDCG}@k$ being the $\textit{DCG}@k$ for a perfect ranking. 

\subsubsection{Reciprocal Rank} 
The metrics described before, score the whole ranking up to rank $k$. This might be unnecessary, if a document found is relevant, it should not matter anymore what the relevance assessment is of all documents after it. After all, the user would stop looking after document $k$, as the relevant information is found. The reciprocal rank only takes the first found relevant document in to account. It divides $1$ by the index of the first found document:
\begin{equation}
	\textit{RR@k} = \frac{1}{\text{minrank}(d)}
\end{equation}
where only the first $k$ documents are considered. If the index of the first relevant is greater than $k$, the resulting score is $0$.

\subsubsection{Significance Testing}
For some of the measures described above, it can still be problematic when they are used to compare different systems. \Citet{fuhr-mrr} describes common mistakes when evaluating IR systems, and how to avoid them.

\section{Data Retrieval}
In the case of information retrieval the concept of \emph{relevance} is very important. When someone issues a query, it does not necessary have to be the case that words in the query have to be present in order for the document to be relevant. Information retrieval has do deal with some uncertainty, and if not all all relevant documents are found this does not necessary have to be a problem. If we would think about a search engine from the perspective of an user, a document is only relevant if it contains the information they are looking for. In order to find this information, the user has to express their information need in a query, which the systems then has to interpret somehow in order to present the user (hopefully) the correction information. 
It might however be possible that two different users issue the same query, e.g.\ \emph{the weather}, but one user is interested in the weather of today, while the other user is interested in what the weather will be tomorrow. In these cases, different documents will be relevant for the user. The concept of relevancy is directly tight to the expectations of the user.

This is not the case in the field of \emph{data retrieval}. When we speak about data retrieval, we are interested in retrieving all data that fulfills some predicate. It assumes that there is no ambiguity in the data. An example could be; give me an list of all dog breeds. In this case, the system should return a list of all breeds of dog, known by the system. If it would accidentally forgot to return one dog breed, or includes an answer that is not correct, the system would work incorrectly. 
The most common used data retrieval systems are relational database management systems (RDBMS), or relational databases. Relational databases are used to represent tabular data as relations. The concept of a relation comes from relational algebra, where a relational is defined as a set of tuples. In a relational database, a table corresponds to the relation, a row is one of tuples, and a column can be considered a attribute in the tuple. There are differences between the theoretic framework of relational algebra, and its implementation in relational databases. For example, in practice its often possible to have duplicate rows in a relational database, which is not possible in a set. 

Relational databases implement the operators as proposed by relational algebra, which means users can select columns from a table (projection), filter rows that fulfill certain predicates (selection), and combine tables (join). Typically, this is done through a structured query language (SQL).  
SQL is a declarative language; in it someone can express which data needs to be retrieved, but not how. So depending on what a user expects from the program, it makes sense to choose different database management systems for different applications. In the case of storing employee records for a company, it makes sense to store this data in a database that is focused on transactions. While if you need to analyze your data for scientistic studies, it makes more sense to store your data in a database system that is built focused on analytics.

\subsection{Physical vs Logical models}
As SQL is a declarative language, the physical and logical models of data management systems are strongly separated. Although many dialects of SQL exists, basic SQL queries are system agnostic. The logical model of data management systems are the SQL queries; what should be done. The physical model is how this should be done, depending on which system is used, things like join ordering and how the data is processed can be very different. The logical model does not ``mind'' how the query is resolved, as long as it is done correctly. 

In the case of information retrieval, the physical and logical models are often more entangled. Model specification and query processing often co-exist, and shortcuts are sometimes taken when these systems are built. When this is the case, it can be harder to detect why systems might differ whey they implement the same models. \Cref{ir-using-relational-databases} will discuss this is in more detail. 

\citet{seperation-logical-physical} discusses how information retrieval might benefit from a separation between physical and logical models in the context of dense retrieval models. \Cref{from-tables-to-graphs} will discuss his proposal in more detail.

\subsection{Columns vs Rows}
Earlier database management systems where row oriented. This means that when we look at how data is physically stored on the computer, all data within a tuple is represented near each other in memory. This orientation is especially well suited when a database needs to process many transactions. Some IR systems have been built on top of such databases, these were however not efficient compared to when inverted indexes where used. Later, column oriented databases were developed. These kinds of databases are more suited for analytical queries, transactions tend however to be more costly. As these databases are more suited for analytics, they are also more suited for information retrieval systems. These systems are more suited for the development IR systems; this is done throughout this thesis. 

\subsection{NoSQL and Graphs}
In recent years there has been some interest in database systems that approach data retrieval differently compared to traditional relational database systems. These systems approach data retrieval not using the relational model. This could for example mean that joins are not possible within the system. This might seem limiting, but if certain operations are not supported it might be possible to increase the efficiency of other operations. This can for example be a very efficient key-value store. 

In particular, graph database systems are NoSQL database management systems that have been of interest in the database community in the last years. The work in~\cref{from-tables-to-graphs} works with these databases.

\subsection{Embedded Database}
Typically, databases are running on dedicated database servers. It is however also possible to built so-called embedded database systems.  

MonetDB~\cite{monet}

DuckDB~\cite{duckdb}

\section{Graphs}
Instead of using columnar data, modeling your data using graphs might be more attractive. 

\section{Reproducible Science}
In order to establish experimental results, it is essential that results can be independently verified. 