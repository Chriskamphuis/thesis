\chapter{Related Work}
\label{related-work}

\epigraph{Machines take me by surprise with great frequency.}{Alan Turing - 1950}

\section{Introduction}
This chapter aims to provide the context in which this work is written. By introducing the related scientific fields, this chapter sketches a broader context wherein this research exists. 

First, the field of information retrieval is introduced, the central area of the research described in this dissertation. Within the field of information retrieval, different approaches to \emph{retrieving information} have been used throughout the decades. Different approaches will be introduced such that the reader understands what works in the field.

Secondly, techniques from different research areas are also used for this research. Specifically, the research described in this paper extensively uses techniques typically investigated by the \emph{data management} community. This field will also be introduced, especially the techniques used for the research described in this work. 

Then, the concept of \emph{graphs} will be explained. Graphs themselves are just mathematical structures that model the relations between objects. The objects are modeled as \emph{nodes} and relations between them as \emph{edges}. Many kinds of graphs exist, and some are very useful for modeling problems that can be expressed formally through this framework. Different kinds of graphs will be explained, and examples of how to use them will be provided.

Finally, we will describe the idea of \emph{reproducible science}. In general, scientific work must be reproducible. In our research, we spend additional effort on reproducible science. What reproducibility exactly entails will be introduced.  

\section{Information Retrieval}
The research described in this thesis is subject to the \emph{information retrieval} field. Colloquially, I would refer to this field as the science of everything related to search engines. This field encompasses all aspects: from user experiences with search engines to storage algorithms and fast retrieval of the information items users search. \Citet{modern-information-retrieval} introduces information retrieval as the following:

\medskip
\textbf{Information retrieval (IR) deals with the representation, storage, organization of, and access to information items.}

\medskip
Following this description, an information retrieval system, i.e., a search engine, is a system that allows users to access information items that they are looking for. How this works internally is typically not interesting for the user, they want to find the information they seek. Typically, and also in this thesis, the term \emph{document} refers to information items, even though the item the user seeks would not be a document in the ``literal sense''. 

Consider the following situation. If one wants to know whether it will rain in the coming half an hour, they might query a web search engine with the text \emph{weather}. In this case, the user does not care how the search engine works but only about the result. In order to satisfy the user's information need, the search engine needs to (1) present the correct weather forecast and (2) do this quickly. If the search engine presents the incorrect weather forecast or cannot do this in seconds, the user will be dissatisfied.

In order to answer this inquiry for information correctly, the information retrieval system needs more context. It can only correctly know the weather if it is known where the user resides. Typically, when accessing search engines through a telephone or computer, this information is sent along with the query as metadata. This way, the search engine can correctly answer even though the user did not explicitly give this information. 

Then when the user's location is known, the search engine needs to find a webpage that correctly provides the weather for that particular location. As there are billions of web pages in the search engine's index, correctly identifying which one contains information about the weather at that particular location and time and then retrieving it in seconds is the next challenge.

Search engines must smartly organize the data because it is infeasible to iterate over all web pages to check if the correct information is available. Typically, the organization of documents within a search engine is done through a data structure called an inverted index.

\subsection{Inverted Indexes}
Inverted indexes have been used for decades in the field of information retrieval. They are designed to access documents quickly. Otherwise, the user would use a different system. Documents are typically structured through words that form sentences, which form paragraphs, and, eventually, stories. When considering a document in its standard form, it is not trivial to quickly know whether it contains a specific word. When searching for something, it is paramount that the search engines know what documents contain the keywords in the query. To store this information, the documents are \emph{inverted}. Instead of storing documents as is; a mapping from documents to words, the search engines store the inverted information; a mapping from words to documents. To provide an example, let us says we have three short documents:

\begin{itemize}
	\item[\textbf{1.}] Cats and dogs are animals.
	\item[\textbf{2.}] Cats are smart animals.
	\item[\textbf{3.}] Dogs are great at tricks. 
\end{itemize}
Then the inverted index looks something like this:

\medskip
\textbf{animal:} [1 2],
\textbf{cat:} [1 2], 
\textbf{dog:} [1 3],
\textbf{great:} 3,
\textbf{smart:} 2,
\textbf{trick:} 3
\medskip

\noindent There are a couple of interesting observations that can be made here. Not all words appear in the inverted index. Typically words without ``meaning'', or so-called stop words, are dropped. These words tend to contain no information and can therefore be dropped. This process is called stopword removal or \emph{stopping}. Then, words are sometimes shortened to their \emph{stem}. Let us say we have a query only mentioning the word ``dog'', so not plural, then we still want to be able to find the documents that mention the plural form. This shortening to the word's stem is called \emph{stemming}. In this case, the words are ordered alphabetically. When the number of documents in a collection increases, the number of unique words also increases. By storing the words alphabetically, a computer can find the entries for that particular word easier. 

When someone queries the search engine with following query: ``dog tricks'', then can directly find which documents contain these words, automatically discarding all documents that do not contains these words. This is done by retrieving the lists associated with these words. These lists are called \emph{posting lists} in the field of information retrieval. Entries in such lists are referred to as \emph{postings}. In this case the postings only contain the document identifiers as data. Generally, however, information like how often a word appears in a document, or its location in the document is also stored in the posting. In practice, posting lists are often compressed and instead of storing the direct identifiers, the gaps between them are stored such that the index becomes smaller. In literature these gaps are referred to as delta-gaps. 

In this case the posting lists of the words ``dog'' and ``trick'' are retrieved, which are [1 3] and [3]. Then some scoring method can process these lists and assign a score to the documents present in these lists. As document 3 is the only document that contains both words, it makes sense to consider this document as the most relevant. However, when multiple documents contain both words with different frequencies, it is not as trivial to assess a relevance ordering. 
To create such an ordering, different models for scoring documents have been proposed in the past. In the following sections these models are presented.

\subsection{Ranking Models}

\subsubsection{Boolean Retrieval}
In the early days of information retrieval, the boolean retrieval model was used. Boolean retrieval can be formulated as the following; let, 

\begin{equation}
	T = \{t_1, \ldots, t_n\}
\end{equation}

be the set of all index terms that appear in the collection. Let,

\begin{equation}
	D = \{d_1, \ldots, d_m\}
\end{equation}

be the set of all documents, where every document is a subset of $T$. Then a query can be any boolean expression over $T$. All documents that adhere to the boolean expression formed by the query are considered relevant. To give an example, consider the same three documents as before:

\begin{itemize}
	\item[\textbf{1.}] Cats and dogs are animals.
	\item[\textbf{2.}] Cats are smart animals.
	\item[\textbf{3.}] Dogs are great at tricks. 
\end{itemize}
Then, $D$ is formulated as $\{d_1, d_2, d_3\}$ with,
\begin{align}
d_1&: \{\mathit{cat}, \textit{dog}, \textit{animal}\} \\
d_2&: \{\mathit{cat}, \textit{smart}, \textit{animal}\} \\
d_3&: \{\mathit{dog}, \textit{great}, \textit{trick}\}
\end{align}

If would be interested in finding all documents that mention dogs, but not cats. Then we can formulate the following boolean query:

\begin{equation}
	Q = \mathit{dog} \land \neg \mathit{cat}
\end{equation}

Then if we find all subsets that adhere to the individual terms in this conjunction we the following set expression:
\begin{equation}
Q = \{d_1, d_3\}\ \cap \{d_3\} = \{d_3\}
\end{equation}
which shows that document $d_3$ is the only document that is about dogs, but not about cats. Although it is possible to express quite complicated expressions with boolean logic, a document either satisfies the expression or it does not. All documents that satisfy the expression are considered relevant; creating a ranking between them cannot be done with this approach alone. 

With this approach there is no clear difference, yet, between data retrieval and information retrieval; there is only one correct solution, namely all documents that satisfy the restrictions imposed by the query. 

\subsubsection{Vector Space Models}
In 1975, the vector space model was introduced by~\citet{VectorSpaceModel}. The idea behind the vector space model is to represent documents and queries as vectors. Consider a collection that has $t$ unique terms, then we can represent a documents in the collection as a $t$ dimensional vector:
\begin{equation}
	D_j = (w_0, \ldots, w_t)
\end{equation}
where $D_j$ is the $j$-th document in the collection, and $w_i$ represents the weight associated with the $i$-th term. These weights can be binary, if one only wants to consider the presence of the term in the document, they can be the term frequency, or any weight that takes into account the ``general'' term importance. 

Then if we have a query $q$, we can represent it in the same way:
\begin{equation}
	q = (q_0, \ldots, q_t)
\end{equation}
where $q_i$ represents the weight of the $i$-th term in the collection. 

Now it is possible to calculate a similarity score between the query and every document. This similarity is typically calculated by calculating the cosine similarity between the two vectors, which between a document $d$ and a query $q$ is defined as:

\begin{equation}
	\cos\left(d, q)\right) = \frac{\textbf{d} \cdot \textbf{q}}{||\textbf{d}||\ ||\textbf{q}||}
\end{equation}

Then, calculating this for the documents in the collection gives a relevance score value for all of them. Ordering them based on relevance, then produces a ranked list where the highest is presumed to be the most interesting for the user. As mentioned before, the weights represented in the document vector do not necessarily have to be binary or the term frequency of the terms in the document. \Citeauthor{VectorSpaceModel} showed that the product of the term frequency with a general term importance worked best. The measure they used for term importance was the inverse document frequency as proposed by \citet{idf} three years earlier, which worked well. 

\subsubsection{Probabilistic Relevance Models}
In 1976,~\citeauthor{RSJ} developed the probabilistic ranking framework. Within this framework, we assume that a document has a certain probability of being relevant given a query. Then, a search systems that implements models within this framework, rank documents where documents with the highest probability of being relevant are ranked the highest.

Within this framework, with the assumption of binary independence, which is nicely explained by ~\citet{bm25-beyond}, the Robertson-Sp{\"a}rck Jones weight can be derived. Which, when assuming no relevance information is available, leads to an approximation of the classical inverse document frequency.

Then by extending this function to include within-document term frequency information and document length normalization, the well known BM25 function can be derived.
This model is the focus of \Cref{ir-using-relational-databases}, and additional information explaining this model will be presented there.

\subsubsection{Language Models}
Later, language model were proposed~\citep{croft_lm, hiemstra_lm, zhai_lm}. The idea behind language models that documents are considered language samples. These samples can be used for a generative process. This process generates terms that exists in the sample by randomly selecting one of the words from that sample. Let the probability of a document $d$ generating a term $t_i$ be:

\begin{equation}
	P(t_i|d) = \frac{\mathit{tf}_{t_i,d}}{\sum_t tf(t, d)}
\end{equation}
with $\mathit{tf}_{t_j, d}$ being the term frequency of term $t_j$ in document $d$. 

Then, if we have a query $Q$. It is possible to calculate the likelihood that this query was generated by the document samples, as we know for every term in the query what the probability is that one of the document samples generates that term: 

\begin{equation}
	P(d|Q) = \prod_{q \in Q} P(q | d)
\end{equation}

When we would just take the document with the maximum likelihood, there are some issues we run into. First of all, if a query contains multiple terms, a document can only get a non-zero likelihood for generating that query if all query terms exist in that document. If this is not the case the probability of generating that term would be 0, and as we are calculating the maximum likelihood we would take the product of the individual probabilities of the query terms. Then the second issue is that there is no term independent specificity weighting, terms that have a higher degree of specificity should increase the resulting probabilities more than ``filler'' words. The previously described inverse document frequency took care of this in the vector space model.

In order to overcome these issues, a smoothing technique tends to be used. Instead of only considering the samples formed by the documents, a collection wide sample is also being used. This collection wide sample generates terms according to the frequency in which the terms appear in the collection:
\begin{equation}
	P(t_i|C) = \frac{\mathit{df}(t_i)}{\sum_t \mathit{df}(t)}
\end{equation}
With $C$ being the collection, and $df(t_i)$ the document frequency of term $t_i$; i.e.\ the number of documents in the collection in which term $t_i$ appears. Then, the probability generated by the document sample is interpolated by the probability generated by the collection sample:
\begin{equation}
	P(d|Q) = \prod_{q \in Q} \omega \cdot P(q | d) + (1 - \omega) \cdot P(q | C) 
\end{equation}
with $\omega$ being the smoothing factor; i.e.\ how much such the collection sample be weighted against the document sample. 

This smoothing solves both problems. As the collection sample contains all terms in the collection, it is not possible anymore for probabilities generated by a certain term to be zero, so the resulting product will not be zero either. Then, words with a higher specificity also get a boost; words with a high specificity have a low probability of being generated by the collection sample. This means that it is more important for a document to contain that word in order to achieve a high probability of relevance. Words that have a low specificity have less effect; the probability of them being generated by the collection sample is already relatively high. 

\subsubsection{Learning to Rank}
With the increase of computing power, learning to rank became a popular approach to ranking. The methods described in the previous sections, can all be considered unsupervised learning methods if we look at from a machine learning perspective. Learning to rank models would be considered supervised learning methods. 

The goal is to learn a model, that given a query ranks documents that are relevant to that query higher than documents that are not relevant to that query. In general, there are three ways to achieve this:

\paragraph{Pointwise.} Pointwise methods are most comparable to standard regression methods. The idea here is to learn a function that tries to predict the relevance label of a document directly. 

The approach of pointwise learning to rank is somewhat naive however, the loss can be very low for a ranking, but the ranking can still be worse than a ranking where the loss is relatively high. %todo cont

\paragraph{Pairwise.} Pairwise ranking tries to deal with this by not directly learning from the relevance label, but by looking at pairs of documents. %todo cont

\paragraph{Listwise.} Instead of only looking at pairs of documents, the listwise approach tries to take the whole ranking as a whole into account. Generally, the idea is to optimize some quality of ranking metric directly. % todo cont

\subsubsection{Vector Space Models revisited}
In the last couple of years the vector space model has been popularized again. 

\subsection{Evaluation}

\section{Relational Databases}
Relational databases are usually used to store structured data. 

\section{Graphs}
Instead of using columnar data, modeling your data using graphs might be more attractive. 

\section{Reproducible Science}
In order to establish experimental results, it is essential that results can be independently verified. 